\subsection{Theoretical Background}
The specific set of rules implemented in this project which limits the side effects to neighboring cells. This rule set is common in game terrain generation but it is also popular because of its locality benefits. 
By locality, it means that non-determinate cells encapsulated by determinate cells will not be restricted further by cells outside of the walls. 
It also means that a map can be divided into smaller sections and allow each processes to handle their local area without depending their calculation on results from other processes. This project will try to take advantage of this task division method to spread the task to all processes in the MPI-connected system.

Moreover, the locality also limits the number of cells to update since their restrictions will be based on cells with Manhattan distance of 9 or below, which means that all cells with the newly defined cell can be updated simultaneously without depending on calculations from the previous updates. This project takes advantage of this and process all cell updates using parallel CUDA threads. In initial testings, using a single block with threads to speed up propagation can achieve some performance gains over CPU but the speedup is limited. Thus, the idea of locality used in the MPI is also adoped in the CUDA implementation to enable the benefit of scheduling CUDA warps;


\subsection{Implementation Details}

First, the map is divided into subsections equal to the number of MPI processes. A minimum height of 9 is set to avoid constraint conflicts. The top horizontal line of each strip are independent from the top horizontal line of other strips because the Manhattan distance is above the limit of the propagation in this rule set.
Thus, the first row of each strip are independently initialized on different processes in the MPI system. Moreover, collapsing cells in a straight line can delay propagation of non-forbidden states in neighboring cells to after the entire edge is initialized.

Then, each process will propagate their top edge to the process responsible for the strip above. This allows for each process to have a localized copy of the problem with a smaller problem size, yet all of them are compatible since the connecting edges are synchronized. Each process have to propagate the constraints from both top and bottom edges before starting to fill in the remaining cells accordingly. 

Each MPI processes perform calculation on CUDA-enabled GPU for parallel constraints propagation. This can be done because constraint propagation in this project is similar to convolution which is suited for SIMD architecture. Continuing the locality idea, each subsection divided by MPI can be further divided into 19 by 19 grids. Given the same local coordinates inside the block, propagation will not overlap with adjacent blocks. 

Lastly, the master rank in the MPI system will collect the strips and combine them into one image which can then be verified or optionally saved to a bitmap file.


The feature to export the resultant map into a 2D image allowed visual confirmation of the correctness of the program on top of the verification implemented that only checks whether the map complies with the rules. This is an important feature not only to allow the computation result to be saved, but also debug versions of the program where a matrix of all zero was returned which can pass the verification but is not the desired result.